# 游늵 Conclusiones y Resultados: Cliff Walking

Este documento resume los hallazgos principales tras la experimentaci칩n en el entorno **Cliff Walking** con los algoritmos **Q-Learning** y **SARSA**.

## 1. Comparativa de Comportamiento

| Caracter칤stica | Q-Learning (Off-Policy) | SARSA (On-Policy) |
| :--- | :--- | :--- |
| **Objetivo** | Aprende la pol칤tica *칩ptima absoluta*. | Aprende la pol칤tica *m치s segura* dado el comportamiento actual. |
| **Riesgo** | **Alto**. Camina pegado al acantilado. | **Bajo**. Se aleja del borde ("Safety Buffer"). |
| **Convergercia** | Converge al camino m치s corto (-13 pasos). | Converge a un camino m치s largo pero seguro. |
| **Rendimiento (Entrenamiento)** | Peor. Sufre muchas ca칤das (-100) por exploraci칩n. | Mejor. Evita ca칤das dr치sticas al ser "consciente" de su torpeza exploratoria. |

## 2. An치lisis de Resultados

### 쯇or qu칠 toman caminos diferentes?
La diferencia clave radica en la ecuaci칩n de actualizaci칩n:

*   **Q-Learning**: `Q(s,a) <-- ... + max Q(s', a')`. Al usar el `max`, el agente asume que en el siguiente paso **no fallar치** y tomar치 la mejor decisi칩n posible. Por eso ve el camino junto al acantilado como el mejor (-13 pasos), ignorando que su exploraci칩n epsilon-greedy podr칤a hacerle caer.
*   **SARSA**: `Q(s,a) <-- ... + Q(s', a')`. Usa la acci칩n que **realmente** va a tomar (que puede ser aleatoria). Si al caminar junto al borde, la exploraci칩n le hace saltar al vac칤o, SARSA asocia "caminar junto al borde" con "dolor", y aprende a evitarlo.

### Gr치ficas Esperadas
En las gr치ficas de entrenamiento (`metrics_comparison.png`), deber칤amos observar:
1.  **SARSA**: Curva de recompensa m치s estable y alta durante el entrenamiento (converge a aprox -30/-50).
2.  **Q-Learning**: Curva con muchos picos hacia abajo (ca칤das) y un promedio peor durante el entrenamiento, aunque su *pol칤tica final* (si quitamos la exploraci칩n) sea te칩ricamente mejor.

## 3. Desaf칤os Superados: Bucles Infinitos

Durante el desarrollo, nos encontramos con que los agentes a veces se quedaban atrapados caminando en c칤rculos en zonas seguras.
*   **Motivo**: Para un agente inexperto, moverse de un lado a otro (-1 por paso) es mejor que arriesgarse a caer al acantilado (-100). Si no encuentra la meta r치pido, prefiere quedarse dando vueltas.
*   **Soluci칩n**: Implementaci칩n de **Safety Locks**:
    *   **Max Steps (1000)**: Fuerza el fin del episodio si se tarda demasiado, obligando al agente a reiniciar y explorar nuevas rutas.

## 4. Conclusi칩n General

En entornos cr칤ticos donde un fallo es catastr칩fico (como un robot real o un acantilado), **SARSA** es preferible para el entrenamiento online porque evita situaciones peligrosas mientras aprende. **Q-Learning**, aunque encuentra la soluci칩n 칩ptima te칩rica, es demasiado arriesgado para aprender "sobre la marcha" en sistemas f칤sicos reales sin simulador.
