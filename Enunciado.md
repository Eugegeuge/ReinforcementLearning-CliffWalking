Pr√°ctica 1 ‚Äì Proyecto de Aprendizaje por
refuerzo.
Manipuladores- Grado en Ingenier√≠a Rob√≥tica
Curso 25/26
1. Introducci√≥n
El objetivo de esta pr√°ctica es que el alumnado adquiera un entendimiento pr√°ctico y
experimental de los algoritmos de Aprendizaje por Refuerzo (RL) estudiados en clase. A
trav√©s de la implementaci√≥n y comparaci√≥n de distintos m√©todos, se busca:
‚Ä¢ Comprender c√≥mo los agentes aprenden en entornos de distinta complejidad.
‚Ä¢ Analizar las fortalezas y limitaciones de los algoritmos Monte Carlo, SARSA y
Q-Learning en un entorno determinado.
‚Ä¢ Observar el efecto de los par√°metros y las pol√≠ticas de acci√≥n sobre el aprendizaje,
incluyendo exploraci√≥n y explotaci√≥n.
‚Ä¢ Aprender a interpretar resultados experimentales para justificar decisiones
t√©cnicas y de dise√±o.
Esta pr√°ctica se realiza utilizando la librer√≠a Gymnasium, que permite simular diferentes
entornos de RL con caracter√≠sticas variadas: determinismo, recompensas escasas,
discretizaci√≥n del espacio, entre otros.
2. Entornos propuestos
Cada grupo (formado por m√°ximo 3 personas) deber√° seleccionar uno de los siguientes
escenarios:
‚Ä¢ Frozen Lake (slippery=True, entorno no determinista)
‚Ä¢ Taxi
‚Ä¢ MountainCar (versi√≥n NO CONTINUA)
‚Ä¢ LunarLander
‚Ä¢ Blackjack
‚Ä¢ CliffWalking (sliperry=True, entorno no determinista)
3. Algoritmos y pol√≠ticas a implementar
Sobre el entorno elegido se deber√°n de implementar y comparar los algoritmos y pol√≠ticas
vistos en clase en base a RESULTADOS OBTENIDOS.
Espec√≠ficamente sobre los algoritmos se debe de hacer una comparativa sobre como
convergen y como de r√°pido aprenden (estimaci√≥n de la funci√≥n de valor), y que
dificultades presentan dado el entorno.
En cuanto a las pol√≠ticas, se debe de justificar la elecci√≥n de la pol√≠tica y analizar su
impacto en la exploraci√≥n, explotaci√≥n y rendimiento del agente en el aprendizaje.
Finalmente, tambien se deben de analizar el efecto de los diferentes par√°metros que
intervienen en el aprendizaje, como la tasa de aprendizaje ùõº, el factor de descuento ùõæ, as√≠
como los par√°metros de las pol√≠ticas ùúÄ de ùúÄ-greedy o ùúè en Softmax.
4. Desarrollo de la pr√°ctica
4.1. Fase inicial (Pr√°ctica 0)
Con el objetivo de comprender bien la din√°mica de aprendizaje de los algoritmos y
pol√≠ticas, se recomienda implementar y depurar todos los algoritmos y pol√≠ticas en el
entorno visto en la pr√°ctica 0, FrozenLake 4x4 (slipeery=False). Esto permite que
observar la convergencia de los algoritmos en un entorno determinista y de baja
complejidad donde realizar pruebas r√°pidas que permitan analizar y aprender la influencia
de los par√°metros en el aprendizaje del agente. Esta parte no es entregable.
Esta parte es opcional‚Ä¶ pero si la ignoras, podr√≠as acabar como en Bihar: ¬°persiguiendo
recompensas y criando serpientes en casa, sin aprender lo que realmente importa !
4.2. Fase experimental
Una vez asegurado el correcto funcionamiento de los algoritmos y su comprensi√≥n, se
ejecutar√°n sobre el entorno seleccionado.
Espec√≠ficamente en esta fase se deber√°:
‚Ä¢ Entrenar un agente en el entorno elegido.
‚Ä¢ Registrar m√©tricas que permitan evaluar el aprendizaje del agente (tasa de √©xito,
episodios hasta convergencia, recompensa acumulada media, estabilidad del
aprendizaje, as√≠ como los aspectos indicados en la practica 0 (recompensas
inmediatas acumuladas, medias, detecci√≥n de estados terminales y condiciones
que lo producen, evoluci√≥n temporal de las observaciones y variables relevantes
del entorno y par√°metros de aprendizaje, as√≠ como cualquier otro aspecto que creas
relevante.
‚Ä¢ Realizar comparativas entre algoritmos y pol√≠ticas.
‚Ä¢ Analizar dificultades del entorno (estocasticidad, recompensas escasas,
exploraci√≥n costosa‚Ä¶).
4.3. Aspectos t√©cnicos a justificar
En documento final a entregar deben aparecer justificados (en base a resultados):
‚Ä¢ La elecci√≥n del algoritmo principal.
‚Ä¢ La pol√≠tica de exploraci√≥n escogida.
‚Ä¢ Los par√°metros finales utilizados.
Adem√°s, se deber√° justificar si ha sido necesario discretizar el entorno (como en el caso
de MountainCar o LunarLander), modificar o redefinir recompensas (reward wrapper), o
si se ha incluido alguna optimizaci√≥n.
5. Entregables
5.1. Exposici√≥n
La exposici√≥n se realizar√° el viernes 19 de diciembre para todos los grupos despu√©s del
examen de teor√≠a. Cada presentaci√≥n debe durar 10-12 minutos. En la presentaci√≥n se
debe de incluir:
‚Ä¢ Explicaci√≥n breve del entorno seleccionado
‚Ä¢ Algoritmos implementados.
‚Ä¢ Resultados experimentales.
‚Ä¢ Comparativas y conclusiones.
‚Ä¢ Demostraci√≥n del agente funcionando.
5.2. Memoria justificativa
La memoria debe de incluir adem√°s de los aspectos t√©cnicos a justificar indicados en la
secci√≥n 4.3, una introducci√≥n breve y descripci√≥n del entorno elegido, as√≠ como el an√°lisis
experimental llevado a cabo especificado en la secci√≥n 4.2.
5.3. C√≥digo
Se debe de entregar el c√≥digo implementado en formato .py y .ipynb (tanto desarrollos
locales como si se utiliza el entorno GoogleColab). Para este √∫ltimo, se deber√° entregar
tambi√©n el enlace de desarrollo.
6. Evaluaci√≥n y entrega
La pr√°ctica se evaluar√° en dos partes, cada una con un peso del 50% sobre la nota final
de pr√°cticas:
‚Ä¢ Presentaci√≥n oral (50%): exposici√≥n del trabajo realizado el d√≠a 19 de diciembre.
‚Ä¢ Memoria explicativa (50%): documento escrito que justifica la implementaci√≥n,
los resultados y las decisiones t√©cnicas.
Ten en cuenta que entre los aspectos principales que se van a evaluar son:
‚Ä¢ Claridad y coherencia en la exposici√≥n y en la documentaci√≥n escrita.
‚Ä¢ Rigor t√©cnico y experimental (experimentaci√≥n llevada a cabo, an√°lisis de
resultados).
‚Ä¢ Capacidad de s√≠ntesis y de explicar los conceptos y resultados de manera
comprensible, justificando la elecci√≥n par√°metros, algoritmos y pol√≠ticas).
‚Ä¢ Eficiencia en el aprendizaje del agente considerando aspectos como la rapidez de
convergencia, estabilidad y calidad de las pol√≠ticas aprendidas.