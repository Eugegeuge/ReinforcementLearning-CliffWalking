{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Práctica de Aprendizaje por Refuerzo: Cliff Walking\n",
                "\n",
                "Este notebook reproduce la funcionalidad de `main.py` para entrenar y comparar agentes SARSA, Q-Learning y Monte Carlo en el entorno Cliff Walking."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gymnasium as gym\n",
                "import numpy as np\n",
                "import sys\n",
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "import json\n",
                "\n",
                "# Asegurarse de que podemos importar desde src\n",
                "import os\n",
                "if os.getcwd() not in sys.path:\n",
                "    sys.path.append(os.getcwd())\n",
                "\n",
                "from src.agent import SarsaAgent, QLearningAgent, MonteCarloAgent\n",
                "from src.utils import plot_metrics, print_policy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_experiment(env, agent, n_episodes=500, max_steps=1000):\n",
                "    rewards = []\n",
                "    steps_list = []\n",
                "    \n",
                "    # Límite de tiempo global de seguridad (5 minutos)\n",
                "    MAX_GLOBAL_TIME = 300 \n",
                "    start_time = time.time()\n",
                "    \n",
                "    # print(f\"Iniciando entrenamiento con seguridad: Timeout global de {MAX_GLOBAL_TIME}s\")\n",
                "    \n",
                "    for episode in range(n_episodes):\n",
                "        # Chequeo de seguridad de tiempo global\n",
                "        current_total_time = time.time() - start_time\n",
                "        if current_total_time > MAX_GLOBAL_TIME:\n",
                "            print(f\"\\n[ALERTA DE SEGURIDAD] Tiempo máximo excedido ({current_total_time:.2f}s > {MAX_GLOBAL_TIME}s).\")\n",
                "            print(\"Abortando entrenamiento y guardando progreso actual...\")\n",
                "            break\n",
                "            \n",
                "        state, _ = env.reset()\n",
                "        action = agent.choose_action(state)\n",
                "        total_reward = 0\n",
                "        terminated = False\n",
                "        truncated = False\n",
                "        steps = 0\n",
                "        \n",
                "        # Log de progreso más frecuente (cada 5%)\n",
                "        log_freq = max(1, n_episodes // 10)\n",
                "        if episode % log_freq == 0:\n",
                "            elapsed = time.time() - start_time\n",
                "            # print(f\"Episodio {episode}/{n_episodes} - Tiempo: {elapsed:.2f}s - Epsilon: {agent.epsilon:.2f}\")\n",
                "        \n",
                "        while not (terminated or truncated) and steps < max_steps:\n",
                "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
                "            \n",
                "            # Choose next action (needed for SARSA, optional for others but good for uniformity)\n",
                "            next_action = agent.choose_action(next_state)\n",
                "            \n",
                "            # Actualizar agente\n",
                "            agent.update(state, action, reward, next_state, next_action)\n",
                "            \n",
                "            state = next_state\n",
                "            action = next_action\n",
                "            total_reward += reward\n",
                "            steps += 1\n",
                "            \n",
                "        rewards.append(total_reward)\n",
                "        steps_list.append(steps)\n",
                "        \n",
                "        # Callback para finalizar episodio\n",
                "        agent.on_episode_end()\n",
                "        \n",
                "    return {'rewards': rewards, 'steps': steps_list}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuración del entorno\n",
                "try:\n",
                "    env = gym.make('CliffWalking-v1', is_slippery=True)\n",
                "except:\n",
                "    print(\"Advertencia: 'is_slippery' no aceptado, usando config por defecto.\")\n",
                "    env = gym.make('CliffWalking-v1')\n",
                "\n",
                "n_states = env.observation_space.n\n",
                "n_actions = env.action_space.n\n",
                "n_episodes = 1000\n",
                "\n",
                "# Definir agentes para comparar\n",
                "agents = {\n",
                "    'SARSA': SarsaAgent(n_states, n_actions),\n",
                "    'Q-Learning': QLearningAgent(n_states, n_actions),\n",
                "    'Monte Carlo': MonteCarloAgent(n_states, n_actions)\n",
                "}\n",
                "\n",
                "all_metrics = {}\n",
                "\n",
                "for name, agent in agents.items():\n",
                "    print(f\"\\n--- Ejecutando {name} ---\")\n",
                "    start_time = time.time()\n",
                "    metrics = run_experiment(env, agent, n_episodes)\n",
                "    elapsed = time.time() - start_time\n",
                "    print(f\"Completado en {elapsed:.2f}s\")\n",
                "    \n",
                "    all_metrics[name] = metrics\n",
                "    \n",
                "    # Mostrar política aprendida\n",
                "    print_policy(agent)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Graficar comparación\n",
                "print(\"\\nGenerando gráfica comparativa...\")\n",
                "plot_metrics(all_metrics)\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}