# PrÃ¡ctica de Aprendizaje por Refuerzo: Cliff Walking

Este repositorio contiene la implementaciÃ³n de soluciones para el entorno **Cliff Walking** de Gymnasium, utilizando algoritmos de Aprendizaje por Refuerzo (RL).

Proyecto realizado para la asignatura de **Manipuladores (Grado en IngenierÃ­a RobÃ³tica)**.

## ğŸ“‹ DescripciÃ³n

El objetivo es entrenar agentes capaces de navegar desde un punto de inicio hasta una meta evitando un "acantilado". Se exploran y comparan tres algoritmos:

*   **Q-Learning** (Off-policy, TD-Control)
*   **SARSA** (On-policy, TD-Control)
*   **Monte Carlo** (First-Visit)

El entorno estÃ¡ configurado como **estocÃ¡stico** (`is_slippery=True`), lo que aÃ±ade incertidumbre a las transiciones.

## ğŸš€ InstalaciÃ³n

1.  Clona este repositorio:
    ```bash
    git clone https://github.com/Eugegeuge/ReinforcementLearning-CliffWalking.git
    cd ReinforcementLearning_CliffWalking
    ```
2.  Instala las dependencias:
    ```bash
    pip install -r requirements.txt
    ```

## ğŸ› ï¸ Uso

Para entrenar al agente SARSA con los mecanismos de seguridad activados, ejecuta el script principal:

```bash
python main.py
```

Esto realizarÃ¡ lo siguiente:
1.  EntrenarÃ¡ al agente **SARSA** durante **1000 episodios**.
2.  GenerarÃ¡ una grÃ¡fica de recompensas (`metrics_comparison.png`).
3.  GuardarÃ¡ la tabla Q (`sarsa_q_table.npy`) y mÃ©tricas (`sarsa_metrics.json`).

## ğŸ›¡ï¸ Mecanismos de Seguridad

Para evitar que el entrenamiento se quede colgado (ej. agente dando vueltas en cÃ­rculos infinitamente), se han implementado:

*   **Timeout Global (300s)**: Si el script tarda mÃ¡s de 5 minutos, se aborta y guarda el progreso.
*   **Max Steps (1000)**: Si un episodio supera los 1000 pasos, se fuerza la terminaciÃ³n de ese episodio.

## ğŸ“‚ Estructura del Proyecto

*   `src/`: CÃ³digo fuente de los agentes (`agent.py`) y utilidades (`utils.py`).
*   `main.py`: Script principal de ejecuciÃ³n y orquestaciÃ³n.
*   `Explicacion_Practica.md`: DocumentaciÃ³n detallada de los algoritmos y justificaciÃ³n teÃ³rica.
*   `Enunciado.md`: DescripciÃ³n original de la prÃ¡ctica.

## ğŸ“Š Resultados Esperados

*   **Q-Learning**: Tiende a aprender el camino Ã³ptimo (pegado al acantilado), pero arriesgado durante el entrenamiento.
*   **SARSA**: Tiende a aprender un camino mÃ¡s seguro (alejado del acantilado) debido a la penalizaciÃ³n por caÃ­das durante la exploraciÃ³n.

## ğŸ‘¥ Autores

*   [Hugo]
*   Hugo LÃ³pez
*   Juan Diego Serrato
