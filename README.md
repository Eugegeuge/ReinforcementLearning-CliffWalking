# ğŸ”ï¸ Cliff Walking - Reinforcement Learning

ImplementaciÃ³n y anÃ¡lisis comparativo de algoritmos de Aprendizaje por Refuerzo en el entorno **Cliff Walking** de Gymnasium.

**PrÃ¡ctica de Manipuladores - Grado en IngenierÃ­a RobÃ³tica**

## ï¿½ Resultados Principales

| Algoritmo | Î± Ã³ptimo | Î³ Ã³ptimo | Îµ Ã³ptimo | Tasa Ã‰xito | Tiempo |
|-----------|----------|----------|----------|------------|--------|
| **SARSA** | 0.1 | 0.99 | 0.01 | **95.6%** | 1.7s |
| **Q-Learning** | 0.1 | 0.99 | 0.01 | **95.4%** | 2.1s |
| **Monte Carlo** | 0.01 | 0.99 | 0.01 | 66.4% | 17.2s |

## ğŸ¯ DescripciÃ³n

El objetivo es entrenar agentes que naveguen desde el inicio (S) hasta la meta (G) evitando el acantilado (C):

```
Start (S)                              Goal (G)
   â†“                                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· â”‚  Fila 0
â”‚ Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· â”‚  Fila 1  
â”‚ Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· â”‚  Fila 2
â”‚ S C C C C C C C C C C G â”‚  Fila 3 (Acantilado)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Entorno estocÃ¡stico**: 10% de probabilidad de acciÃ³n aleatoria (slippery).

## ğŸ§  Algoritmos Implementados

### SARSA (On-policy TD)
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Q(s',a') - Q(s,a)]
```
- Aprende de la acciÃ³n que **realmente toma** (incluyendo exploraciÃ³n)
- MÃ¡s conservador, evita el acantilado

### Q-Learning (Off-policy TD)
```
Q(s,a) â† Q(s,a) + Î±[r + Î³Â·max_a'Q(s',a') - Q(s,a)]
```
- Aprende la polÃ­tica **Ã³ptima** independiente de exploraciÃ³n
- MÃ¡s agresivo, puede subestimar riesgos

### Monte Carlo (First-Visit)
```
G â† retorno acumulado desde el final del episodio
Q(s,a) â† Q(s,a) + Î±[G - Q(s,a)]
```
- Actualiza solo al **final del episodio**
- Alta varianza, lento en entornos estocÃ¡sticos

## ğŸ“ Estructura del Proyecto

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent.py              # Clases base de agentes RL
â”‚   â””â”€â”€ utils.py              # Utilidades y visualizaciÃ³n
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ estudios/             # Estudios de hiperparÃ¡metros
â”‚   â”‚   â”œâ”€â”€ *_epsilon_study.py
â”‚   â”‚   â”œâ”€â”€ *_alpha_study.py
â”‚   â”‚   â””â”€â”€ *_gamma_study.py
â”‚   â”‚
â”‚   â”œâ”€â”€ optimos/              # Entrenamientos con config Ã³ptima
â”‚   â”‚   â”œâ”€â”€ montecarlo_optimo.py
â”‚   â”‚   â”œâ”€â”€ sarsa_optimo.py
â”‚   â”‚   â””â”€â”€ qlearning_optimo.py
â”‚   â”‚
â”‚   â”œâ”€â”€ comparaciones/        # Comparaciones entre modelos
â”‚   â””â”€â”€ utilidades/           # Scripts auxiliares
â”‚
â”œâ”€â”€ graphs/
â”‚   â”œâ”€â”€ montecarlo/           # GrÃ¡ficos + documentaciÃ³n MC
â”‚   â”œâ”€â”€ sarsa/                # GrÃ¡ficos + documentaciÃ³n SARSA
â”‚   â”œâ”€â”€ qlearning/            # GrÃ¡ficos + documentaciÃ³n Q-Learning
â”‚   â””â”€â”€ comparacion_todos/    # Comparaciones generales
â”‚
â””â”€â”€ main.py                   # Script principal
```

## ğŸš€ InstalaciÃ³n

```bash
git clone https://github.com/Eugegeuge/ReinforcementLearning-CliffWalking.git
cd ReinforcementLearning-CliffWalking
pip install -r requirements.txt
```

## ğŸ› ï¸ Uso

### Ejecutar estudios de parÃ¡metros
```bash
python scripts/estudios/sarsa_epsilon_study.py
python scripts/estudios/montecarlo_alpha_study.py
```

### Entrenar con configuraciÃ³n Ã³ptima
```bash
python scripts/optimos/sarsa_optimo.py
python scripts/optimos/qlearning_optimo.py
python scripts/optimos/montecarlo_optimo.py
```

### Comparar todos los modelos
```bash
python scripts/comparaciones/run_full_training.py
```

## ï¿½ Hallazgos Clave

### Por quÃ© SARSA/Q-Learning superan a Monte Carlo en CliffWalking:

1. **ActualizaciÃ³n paso a paso**: Los mÃ©todos TD propagan el conocimiento inmediatamente
2. **Menor varianza**: No acumulan error de todo el episodio
3. **Tolerancia a Î± mÃ¡s alto**: Pueden usar Î±=0.1 vs Î±=0.01 de MC

### ParÃ¡metros Ã³ptimos encontrados:

| ParÃ¡metro | SARSA | Q-Learning | Monte Carlo |
|-----------|-------|------------|-------------|
| **Alpha** | 0.1 | 0.1 | 0.01 |
| **Gamma** | 0.99 | 0.99 | 0.99 |
| **Epsilon** | 0.01 | 0.01 | 0.01 |
| **Episodios** | 10K | 10K | 15K |

## ğŸ“– DocumentaciÃ³n

Ver justificaciÃ³n detallada de cada parÃ¡metro en:
- [`graphs/sarsa/JUSTIFICACION_PARAMETROS_SARSA.md`](graphs/sarsa/JUSTIFICACION_PARAMETROS_SARSA.md)
- [`graphs/qlearning/JUSTIFICACION_PARAMETROS_QLEARNING.md`](graphs/qlearning/JUSTIFICACION_PARAMETROS_QLEARNING.md)
- [`graphs/montecarlo/JUSTIFICACION_PARAMETROS_MC.md`](graphs/montecarlo/JUSTIFICACION_PARAMETROS_MC.md)

## ğŸ‘¥ Autores

- Hugo Sevilla
- Hugo LÃ³pez
- Juan Diego Serrato

---

**Universidad de Alicante - Grado en IngenierÃ­a RobÃ³tica**
