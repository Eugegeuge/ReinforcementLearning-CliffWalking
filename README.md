# PrÃ¡ctica de Aprendizaje por Refuerzo: Cliff Walking

Este repositorio contiene la implementaciÃ³n de soluciones para el entorno **Cliff Walking** de Gymnasium, utilizando algoritmos de Aprendizaje por Refuerzo (RL).

Proyecto realizado para la asignatura de **Manipuladores (Grado en IngenierÃ­a RobÃ³tica)**.

## ğŸ“‹ DescripciÃ³n

El objetivo es entrenar agentes capaces de navegar desde un punto de inicio hasta una meta evitando un "acantilado". Se exploran y comparan tres algoritmos:

*   **Q-Learning** (Off-policy, TD-Control)
*   **SARSA** (On-policy, TD-Control)
*   **Monte Carlo** (First-Visit)

El entorno estÃ¡ configurado como **estocÃ¡stico** (`is_slippery=True`), lo que aÃ±ade incertidumbre a las transiciones.

## ğŸš€ InstalaciÃ³n

1.  Clona este repositorio:
    ```bash
    git clone <URL_DEL_REPOSITORIO>
    cd ReinforcementLearning_CliffWalking
    ```
2.  Instala las dependencias:
    ```bash
    pip install -r requirements.txt
    ```

## ğŸ› ï¸ Uso

Para entrenar los agentes y generar las comparativas, ejecuta el script principal:

```bash
python main.py
```

Esto realizarÃ¡ lo siguiente:
1.  EntrenarÃ¡ a los tres agentes durante 500 episodios.
2.  GenerarÃ¡ una grÃ¡fica de recompensas (`rewards.png`).
3.  ImprimirÃ¡ por consola las polÃ­ticas aprendidas.

## ğŸ“‚ Estructura del Proyecto

*   `src/`: CÃ³digo fuente de los agentes (`agent.py`) y utilidades (`utils.py`).
*   `main.py`: Script principal de ejecuciÃ³n y orquestaciÃ³n.
*   `Explicacion_Practica.md`: DocumentaciÃ³n detallada de los algoritmos y justificaciÃ³n teÃ³rica.
*   `Enunciado.md`: DescripciÃ³n original de la prÃ¡ctica.

## ğŸ“Š Resultados Esperados

*   **Q-Learning**: Tiende a aprender el camino Ã³ptimo (pegado al acantilado), pero arriesgado durante el entrenamiento.
*   **SARSA**: Tiende a aprender un camino mÃ¡s seguro (alejado del acantilado) debido a la penalizaciÃ³n por caÃ­das durante la exploraciÃ³n.

## ğŸ‘¥ Autores

*   [Hugo]
